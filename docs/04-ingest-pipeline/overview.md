# Lab 04 - Ingest Data with a Pipeline in Microsoft Fabric

A data lakehouse is a common analytical data store for cloud-scale analytics solutions. One of the core tasks of a data engineer is to implement and manage the ingestion of data from multiple operational data sources into the lakehouse. In Microsoft Fabric, you can implement *extract, transform, and load* (ETL) or *extract, load, and transform* (ELT) solutions for data ingestion through the creation of *pipelines*.

Fabric also supports Apache Spark, enabling you to write and run code to process data at scale. By combining the pipeline and Spark capabilities in Fabric, you can implement complex data ingestion logic that copies data from external sources into the OneLake storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis.

This lab will take approximately **45** minutes to complete.

### Step 1: Signing in to Microsoft Fabric

- In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.

### Step 2: Create a workspace

- Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.

### Step 3: Create a lakehouse

- Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.

### Step 4: Create a pipeline

- A simple way to ingest data is to use a **Copy Data** activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.
- For this exercise we will copy a `sales.csv` file

### Step 5: Create a notebook

- The next step is to load the data from the `sales.csv` file that was ingested by the **Copy Data** activity, apply some transformation logic, and save the transformed data as a table. 
- You will append the data if the table already exists.

### Step 6: Modify the pipeline

- Now that you've implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.

---
