{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MS Fabric Labs","text":"<p>This site hosts the instructions for MS Fabric labs:</p> <ul> <li>01 - Create a Microsoft Fabric Lakehouse</li> </ul>"},{"location":"01-lakehouse/","title":"Lab 01 - Create a Microsoft Fabric Lakehouse","text":"<p>Large-scale data analytics solutions have traditionally been built around a data warehouse, in which data is stored in relational tables and queried using SQL. The growth in \u201cbig data\u201d (characterized by high volumes, variety, and velocity of new data assets) together with the availability of low-cost storage and cloud-scale distributed compute technologies has led to an alternative approach to analytical data storage; the data lake. In a data lake, data is stored as files without imposing a fixed schema for storage. Increasingly, data engineers and analysts seek to benefit from the best features of both of these approaches by combining them in a data lakehouse; in which data is stored in files in a data lake and a relational schema is applied to them as a metadata layer so that they can be queried using traditional SQL semantics.</p> <p>In Microsoft Fabric, a lakehouse provides highly scalable file storage in a OneLake store (built on Azure Data Lake Store Gen2) with a metastore for relational objects such as tables and views based on the open source Delta Lake table format. Delta Lake enables you to define a schema of tables in your lakehouse that you can query using SQL.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"01-lakehouse/#overview","title":"Overview","text":""},{"location":"01-lakehouse/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<ul> <li>Sign in to Microsoft Fabric using the email and password from the QA Platform.</li> </ul>"},{"location":"01-lakehouse/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<ul> <li>Before working with data in Fabric, you need to create a workspace.</li> </ul>"},{"location":"01-lakehouse/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<ul> <li>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</li> <li>Currently, there are no tables or files in this lakehouse.</li> </ul>"},{"location":"01-lakehouse/#step-4-upload-a-file","title":"Step 4: Upload a file","text":"<p>Fabric provides multiple ways to load data into the lakehouse, including built-in support for pipelines that copy data from external sources and data flows (Gen 2) that you can define using visual tools based on Power Query. However one of the simplest ways to ingest small amounts of data is to upload files or folders from your local computer (or lab VM if applicable).</p> <p>In this lab we will upload a file ~ <code>sales.csv</code></p>"},{"location":"01-lakehouse/#step-5-explore-shortcuts","title":"Step 5: Explore shortcuts","text":"<p>In many scenarios, the data you need to work with in your lakehouse may be stored in some other location. While there are many ways to ingest data into the OneLake storage for your lakehouse, another option is to instead create a shortcut. Shortcuts enable you to include externally sourced data in your analytics solution without the overhead and risk of data inconsistency associated with copying it.</p>"},{"location":"01-lakehouse/#step-6-load-file-data-into-a-table","title":"Step 6: Load file data into a table","text":"<p>The sales data you uploaded is in a file, which you can work with directly by using Apache Spark code. However, in many scenarios you may want to load the data from the file into a table so that you can query it using SQL.</p>"},{"location":"01-lakehouse/#step-7-use-sql-to-query-tables","title":"Step 7: Use SQL to query tables","text":"<p>When you create a lakehouse and define tables in it, a SQL endpoint is automatically created through which the tables can be queried using SQL <code>SELECT</code> statements.</p>"},{"location":"01-lakehouse/#step-8-create-a-visual-query","title":"Step 8: Create a visual query","text":"<p>While many data professionals are familiar with SQL, those with Power BI experience can apply their Power Query skills to create visual queries.</p>"},{"location":"01-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse and imported data into it. You've seen how a lakehouse consists of files and tables stored in a OneLake data store. The managed tables can be queried using SQL, and are included in a default semantic model to support data visualizations.</p> <p>Once you've finished exploring your lakehouse, you should delete the workspace you created for this exercise.</p>"},{"location":"01-lakehouse/instructions/","title":"Lab 01 ~ Create a Microsoft Fabric Lakehouse","text":"<p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"01-lakehouse/instructions/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>View the new lakehouse, and note that the Lakehouse explorer pane on the left enables you to browse tables and files in the lakehouse:</p> <ul> <li> <p>The Tables folder contains tables that you can query using SQL semantics. Tables in a Microsoft Fabric lakehouse are based on the open source Delta Lake file format, commonly used in Apache Spark.</p> </li> <li> <p>The Files folder contains data files in the OneLake storage for the lakehouse that aren't associated with managed delta tables. You can also create shortcuts in this folder to reference data that is stored externally.</p> </li> </ul> </li> </ol> <p>Currently, there are no tables or files in this lakehouse.</p>"},{"location":"01-lakehouse/instructions/#step-4-upload-a-file","title":"Step 4: Upload a file","text":"<p>Fabric provides multiple ways to load data into the lakehouse, including built-in support for pipelines that copy data from external sources and data flows (Gen 2) that you can define using visual tools based on Power Query. However one of the simplest ways to ingest small amounts of data is to upload files or folders from your local computer (or lab VM if applicable).</p> <ol> <li> <p>Download the sales.csv file from https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv, </p> <ul> <li>Save it as <code>sales.csv</code> on your local computer (or lab VM if applicable).</li> </ul> <p>Note</p> <ul> <li>To download the file, open a new tab in the browser and paste in the URL.</li> <li>Right click anywhere on the page containing the data and select \"Save as\" to save the data as a CSV file.</li> </ul> </li> <li> <p>Return to the web browser tab containing your lakehouse</p> <ul> <li>Click the ... menu for the Files folder in the Explorer pane select New subfolder</li> <li>Name the new subfolder: <code>data</code></li> <li>Click Create</li> </ul> </li> <li> <p>In the ... menu for the new data folder, select Upload and Upload files.</p> <ul> <li>Then upload the sales.csv file from your local computer (or lab VM if applicable).</li> </ul> </li> <li> <p>After the file has been uploaded, select the Files/data folder and verify that the sales.csv file has been uploaded, as shown here:</p> <p></p> </li> <li> <p>Select the sales.csv file to see a preview of its contents.</p> <p>If the sales.csv file does not automatically appear, in the ... menu for the data folder, select Refresh.</p> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-5-explore-shortcuts","title":"Step 5: Explore shortcuts","text":"<p>In many scenarios, the data you need to work with in your lakehouse may be stored in some other location. While there are many ways to ingest data into the OneLake storage for your lakehouse, another option is to instead create a shortcut. Shortcuts enable you to include externally sourced data in your analytics solution without the overhead and risk of data inconsistency associated with copying it.</p> <ol> <li> <p>In the ... menu for the Files folder, select New shortcut.</p> </li> <li> <p>View the available data source types for shortcuts.</p> <ul> <li>Then close the New shortcut dialog box without creating a shortcut.</li> </ul> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-6-load-file-data-into-a-table","title":"Step 6: Load file data into a table","text":"<p>The sales data you uploaded is in a file, which you can work with directly by using Apache Spark code. However, in many scenarios you may want to load the data from the file into a table so that you can query it using SQL.</p> <ol> <li> <p>In the Explorer pane, select the Files/data folder so you can see the sales.csv file it contains.</p> </li> <li> <p>In the ... menu for the sales.csv file, select Load to Tables &gt; New table.</p> <p></p> </li> <li> <p>In Load to table dialog box, set the table name to sales and confirm the load operation.</p> <ul> <li>Then wait for the table to be created and loaded.</li> </ul> <p>If the sales table does not automatically appear, in the ... menu for the Tables folder, select Refresh.</p> </li> <li> <p>In the Explorer pane, select the sales table that has been created to view the data:</p> <p></p> </li> <li> <p>In the ... menu for the sales table, select View files to see the underlying files for this table:</p> <p></p> <p>Files for a delta table are stored in Parquet format, and include a subfolder named <code>_delta_log</code> in which details of transactions applied to the table are logged.</p> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-7-use-sql-to-query-tables","title":"Step 7: Use SQL to query tables","text":"<p>When you create a lakehouse and define tables in it, a SQL endpoint is automatically created through which the tables can be queried using SQL <code>SELECT</code> statements.</p> <ol> <li> <p>At the top-right of the Lakehouse page, switch from Lakehouse to SQL analytics endpoint.</p> <ul> <li>Then wait a short time until the SQL analytics endpoint for your lakehouse opens in a visual interface from which you can query its tables.</li> </ul> </li> <li> <p>Use the New SQL query button to open a new query editor, and enter the following SQL query:</p> <pre><code>SELECT Item, SUM(Quantity * UnitPrice) AS Revenue\nFROM sales\nGROUP BY Item\n</code></pre> </li> <li> <p>Use the  Run button to run the query and view the results, which should show the total revenue for each product.</p> <p></p> </li> </ol>"},{"location":"01-lakehouse/instructions/#step-8-create-a-visual-query","title":"Step 8: Create a visual query","text":"<p>While many data professionals are familiar with SQL, those with Power BI experience can apply their Power Query skills to create visual queries.</p> <ol> <li> <p>On the toolbar, expand the New SQL query option and select New visual query.</p> </li> <li> <p>Drag the sales table (under dbo &gt; Tables) to the new visual query editor pane that opens to create a Power Query as shown here:</p> <p></p> </li> <li> <p>In the Manage columns menu, select Choose columns.</p> <ul> <li>Then select only the SalesOrderNumber and SalesOrderLineNumber columns. Click OK</li> </ul> <p></p> </li> <li> <p>in the Transform menu, select Group by. Then group the data by using the following Basic settings:</p> <ul> <li>Group by: SalesOrderNumber</li> <li>New column name: <code>LineItems</code></li> <li>Operation: Count distinct values</li> <li>Column: SalesOrderLineNumber (if not greyed out)</li> </ul> <p>When you're done, the results pane under the visual query shows the number of line items for each sales order.</p> <p></p> </li> </ol>"},{"location":"01-lakehouse/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse and imported data into it. You've seen how a lakehouse consists of files and tables stored in a OneLake data store. The managed tables can be queried using SQL, and are included in a default semantic model to support data visualizations.</p> <p>Once you've finished exploring your lakehouse, you should delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/01-lakehouse.html </p>"},{"location":"03b-medallion-lakehouse/","title":"Lab 03b - Create Medallion Architecture in a Fabric Lakehouse","text":"<p>In this exercise you will build out a medallion architecture in a Fabric lakehouse using notebooks. You will create a workspace, create a lakehouse, upload data to the bronze layer, transform the data and load it to the silver Delta table, transform the data further and load it to the gold Delta tables, and then explore the semantic model and create relationships.</p> <p>This exercise should take approximately 45 minutes to complete</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"03b-medallion-lakehouse/#overview","title":"Overview","text":""},{"location":"03b-medallion-lakehouse/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>For this lab, you must sign in to Microsoft Fabric using the email and password from the QA Platform.</p>"},{"location":"03b-medallion-lakehouse/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, create a workspace with the Fabric trial enabled.</p>"},{"location":"03b-medallion-lakehouse/#step-3-create-a-lakehouse-and-upload-data-to-bronze-layer","title":"Step 3: Create a lakehouse and upload data to bronze layer","text":"<p>Now that you have a workspace, it\u2019s time to create a data lakehouse for the data you\u2019re going to analyze.</p>"},{"location":"03b-medallion-lakehouse/#step-4-transform-data-and-load-to-silver-delta-table","title":"Step 4: Transform data and load to silver Delta table","text":"<p>Now that you have some data in the bronze layer of your lakehouse, you can use a notebook to transform the data and load it to a delta table in the silver layer.</p>"},{"location":"03b-medallion-lakehouse/#step-5-explore-data-in-the-silver-layer-using-the-sql-endpoint","title":"Step 5: Explore data in the silver layer using the SQL endpoint","text":"<p>Now that you have data in your silver layer, you can use the SQL analytics endpoint to explore the data and perform some basic analysis.</p>"},{"location":"03b-medallion-lakehouse/#step-6-transform-data-for-gold-layer","title":"Step 6: Transform data for gold layer","text":"<p>You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you\u2019ll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.</p>"},{"location":"03b-medallion-lakehouse/#step-7-optional-create-a-semantic-model","title":"Step 7: (OPTIONAL) Create a semantic model","text":"<p>In your workspace, you can now use the gold layer to create a report and analyze the data. You can access the semantic model directly in your workspace to create relationships and measures for reporting.</p>"},{"location":"03b-medallion-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you\u2019ve learned how to create a medallion architecture in a Microsoft Fabric lakehouse.</p> <p>If you\u2019ve finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p>"},{"location":"03b-medallion-lakehouse/instructions/","title":"Lab 03b ~ Create Medallion Architecture in a Fabric Lakehouse","text":"<p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"03b-medallion-lakehouse/instructions/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"03b-medallion-lakehouse/instructions/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> <li> <p>Navigate to the workspace settings and verify that the Data model settings preview feature is enabled. This will enable you to create relationships between tables in your lakehouse using a Power BI semantic model.</p> <p></p> <p>You may need to refresh the browser tab after enabling the preview feature.</p> </li> </ol>"},{"location":"03b-medallion-lakehouse/instructions/#step-3-create-a-lakehouse-and-upload-data-to-bronze-layer","title":"Step 3: Create a lakehouse and upload data to bronze layer","text":"<p>Now that you have a workspace, it's time to create a data lakehouse for the data you're going to analyze.</p> <ol> <li> <p>In the workspace you just created, create a new Lakehouse named Sales by selecting the + New item button.</p> <p>After a minute or so, a new empty lakehouse will be created. Next, you'll ingest some data into the data lakehouse for analysis. There are multiple ways to do this, but in this exercise you'll simply download a text file to your local computer (or lab VM if applicable) and then upload it to your lakehouse.</p> </li> <li> <p>Download the data file for this exercise from <code>https://github.com/MicrosoftLearning/dp-data/blob/main/orders.zip</code> Extract the files and save them with their original names on your local computer (or lab VM if applicable).</p> <p>There should be 3 files containing sales data for 3 years: 2019.csv, 2020.csv, and 2021.csv</p> </li> <li> <p>Return to the web browser tab containing your lakehouse, and in the ... menu for the Files folder in the Explorer pane, select New subfolder and create a folder named bronze.</p> </li> <li> <p>In the ... menu for the bronze folder, select Upload and Upload files, and then upload the 3 files (2019.csv, 2020.csv, and 2021.csv) from your local computer (or lab VM if applicable) to the lakehouse. Use the shift key to upload all 3 files at once.</p> </li> <li> <p>After the files have been uploaded, select the bronze folder; and verify that the files have been uploaded, as shown here:</p> <p></p> </li> </ol>"},{"location":"03b-medallion-lakehouse/instructions/#step-4-transform-data-and-load-to-silver-delta-table","title":"Step 4: Transform data and load to silver Delta table","text":"<p>Now that you have some data in the bronze layer of your lakehouse, you can use a notebook to transform the data and load it to a delta table in the silver layer.</p> <ol> <li> <p>On the Home page while viewing the contents of the bronze folder in your data lake, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open.</p> </li> <li> <p>When the notebook opens, rename it to <code>Transform data for Silver</code> by selecting the <code>Notebook xxxx</code> text at the top left of the notebook and entering the new name.</p> <p></p> </li> <li> <p>Select the existing cell in the notebook, which contains some simple commented-out code.</p> <p>Highlight and delete the two lines - you will not need this code.</p> <p>Note: In this exercise, you'll use PySpark and SQL.</p> </li> <li> <p>Paste the following code into the cell:</p> <pre><code>from pyspark.sql.types import *\n\n# Create the schema for the table\norderSchema = StructType([\n    StructField(\"SalesOrderNumber\", StringType()),\n    StructField(\"SalesOrderLineNumber\", IntegerType()),\n    StructField(\"OrderDate\", DateType()),\n    StructField(\"CustomerName\", StringType()),\n    StructField(\"Email\", StringType()),\n    StructField(\"Item\", StringType()),\n    StructField(\"Quantity\", IntegerType()),\n    StructField(\"UnitPrice\", FloatType()),\n    StructField(\"Tax\", FloatType())\n    ])\n\n# Import all files from bronze folder of lakehouse\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderSchema).load(\"Files/bronze/*.csv\")\n\n# Display the first 10 rows of the dataframe to preview your data\ndisplay(df.head(10))\n</code></pre> </li> <li> <p>Use the  (Run cell) button on the left of the cell to run the code.</p> <p>Note</p> <ul> <li>Since this is the first time you've run any Spark code in this notebook, a Spark session must be started.</li> <li>This means that the first run can take a minute or so to complete. </li> <li>Subsequent runs will be quicker.</li> </ul> </li> <li> <p>When the cell command has completed, review the output below the cell, which should look similar to this:</p> <pre><code>| Index | SalesOrderNumber | SalesOrderLineNumber | OrderDate  | CustomerName | Email             | Item                 | Quantity | UnitPrice | Tax      |\n| ----- | ---------------- | -------------------- | ---------- | ------------ | ----------------- | -------------------- | -------- | --------- | -------- |\n| 1     | SO49172          | 1                    | 2021-01-01 | Brian Howard | brian@example.com | Road-250 Red, 52     | 1        | 2443.35   | 195.468  |\n| 2     | SO49173          | 1                    | 2021-01-01 | Linda Alvarez| linda@example.com | Mount-200 Silver, 38 | 1        | 2071.4197 | 165.7136 |\n| ...   | ...              | ...                  | ...        | ...          | ...               | ...                  | ...      | ...       | ...      |\n</code></pre> <p>The code you ran loaded the data from the CSV files in the bronze folder into a Spark dataframe, and then displayed the first few rows of the dataframe.</p> <p>Note: You can clear, hide, and auto-resize the contents of the cell output by selecting the ... menu at the top left of the output pane.</p> </li> <li> <p>Now you'll add columns for data validation and cleanup, using a PySpark dataframe to add columns and update the values of some of the existing columns. Use the + Code button to add a new code block and add the following code to the cell:</p> <pre><code>from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n\n# Add columns IsFlagged, CreatedTS and ModifiedTS\ndf = df.withColumn(\"FileName\", input_file_name()) \\\n    .withColumn(\"IsFlagged\", when(col(\"OrderDate\") &lt; '2019-08-01',True).otherwise(False)) \\\n    .withColumn(\"CreatedTS\", current_timestamp()).withColumn(\"ModifiedTS\", current_timestamp())\n\n# Update CustomerName to \"Unknown\" if CustomerName null or empty\ndf = df.withColumn(\"CustomerName\", when((col(\"CustomerName\").isNull() | (col(\"CustomerName\")==\"\")),lit(\"Unknown\")).otherwise(col(\"CustomerName\")))\n</code></pre> <ul> <li>The first line of the code imports the necessary functions from PySpark. </li> <li>You're then adding new columns to the dataframe so you can track the source file name, whether the order was flagged as being a before the fiscal year of interest, and when the row was created and modified.</li> <li>Finally, you're updating the CustomerName column to \"Unknown\" if it's null or empty.</li> </ul> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> </li> <li> <p>Next, you'll define the schema for the sales_silver table in the sales database using Delta Lake format. Create a new code block and add the following code to the cell:</p> <pre><code># Define the schema for the sales_silver table\n\nfrom pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.sales_silver\") \\\n    .addColumn(\"SalesOrderNumber\", StringType()) \\\n    .addColumn(\"SalesOrderLineNumber\", IntegerType()) \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"CustomerName\", StringType()) \\\n    .addColumn(\"Email\", StringType()) \\\n    .addColumn(\"Item\", StringType()) \\\n    .addColumn(\"Quantity\", IntegerType()) \\\n    .addColumn(\"UnitPrice\", FloatType()) \\\n    .addColumn(\"Tax\", FloatType()) \\\n    .addColumn(\"FileName\", StringType()) \\\n    .addColumn(\"IsFlagged\", BooleanType()) \\\n    .addColumn(\"CreatedTS\", DateType()) \\\n    .addColumn(\"ModifiedTS\", DateType()) \\\n    .execute()\n</code></pre> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> </li> <li> <p>Select the ... in the Tables section of the Explorer pane and select Refresh. You should now see the new sales_silver table listed. The  (triangle icon) indicates that it's a Delta table.</p> <p>Note: If you don't see the new table, wait a few seconds and then select Refresh again, or refresh the entire browser tab.</p> </li> <li> <p>Now you're going to perform an upsert operation on a Delta table, updating existing records based on specific conditions and inserting new records when no match is found. Add a new code block and paste the following code:</p> <pre><code># Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.\n\nfrom delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')\n\ndfUpdates = df\n\ndeltaTable.alias('silver') \\\n  .merge(\n    dfUpdates.alias('updates'),\n    'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'\n  ) \\\n  .whenMatchedUpdate(set =\n    {\n\n    }\n  ) \\\n.whenNotMatchedInsert(values =\n    {\n      \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n      \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n      \"OrderDate\": \"updates.OrderDate\",\n      \"CustomerName\": \"updates.CustomerName\",\n      \"Email\": \"updates.Email\",\n      \"Item\": \"updates.Item\",\n      \"Quantity\": \"updates.Quantity\",\n      \"UnitPrice\": \"updates.UnitPrice\",\n      \"Tax\": \"updates.Tax\",\n      \"FileName\": \"updates.FileName\",\n      \"IsFlagged\": \"updates.IsFlagged\",\n      \"CreatedTS\": \"updates.CreatedTS\",\n      \"ModifiedTS\": \"updates.ModifiedTS\"\n    }\n  ) \\\n  .execute()\n</code></pre> </li> <li> <p>Run the cell to execute the code using the  (Run cell) button.</p> <p>This operation is important because it enables you to update existing records in the table based on the values of specific columns, and insert new records when no match is found. This is a common requirement when you're loading data from a source system that may contain updates to existing and new records.</p> </li> </ol> <p>You now have data in your silver delta table that is ready for further transformation and modeling.</p>"},{"location":"03b-medallion-lakehouse/instructions/#step-5-explore-data-in-the-silver-layer-using-the-sql-endpoint","title":"Step 5: Explore data in the silver layer using the SQL endpoint","text":"<p>Now that you have data in your silver layer, you can use the SQL analytics endpoint to explore the data and perform some basic analysis. This is useful if you're familiar with SQL and want to do some basic exploration of your data. In this exercise we're using the SQL endpoint view in Fabric, but you can use other tools like SQL Server Management Studio (SSMS) and Azure Data Explorer.</p> <ol> <li> <p>Navigate back to your workspace and notice that you now have several items listed. Select the Sales SQL analytics endpoint to open your lakehouse in the SQL analytics endpoint view.</p> <p></p> </li> <li> <p>Select New SQL query from the ribbon, which will open a SQL query editor. Note that you can rename your query using the ... menu item next to the existing query name in the Explorer pane.</p> <p>Next, you'll run two sql queries to explore the data.</p> </li> <li> <p>Paste the following query into the query editor and select Run:</p> <pre><code>SELECT YEAR(OrderDate) AS Year\n    , CAST (SUM(Quantity * (UnitPrice + Tax)) AS DECIMAL(12, 2)) AS TotalSales\nFROM sales_silver\nGROUP BY YEAR(OrderDate) \nORDER BY YEAR(OrderDate)\n</code></pre> <p>This query calculates the total sales for each year in the sales_silver table. Your results should look like this:</p> <p></p> </li> <li> <p>Next you'll review which customers are purchasing the most (in terms of quantity). Paste the following query into the query editor and select Run:</p> <pre><code>SELECT TOP 10 CustomerName, SUM(Quantity) AS TotalQuantity\nFROM sales_silver\nGROUP BY CustomerName\nORDER BY TotalQuantity DESC\n</code></pre> <p>This query calculates the total quantity of items purchased by each customer in the sales_silver table, and then returns the top 10 customers in terms of quantity.</p> <p>Note: Data exploration at the Silver layer is useful for basic analysis ...</p> <ul> <li>But you need to transform the data further and model it into a star schema.</li> <li>This will enable you to do more advanced analysis and reporting.</li> <li>You'll do that in the next section.</li> </ul> </li> </ol>"},{"location":"03b-medallion-lakehouse/instructions/#step-6-transform-data-for-gold-layer","title":"Step 6: Transform data for gold layer","text":"<p>You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you'll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.</p> <p>Note: You could have done all of this in a single notebook ...</p> <ul> <li>But for this exercise you're using separate notebooks</li> <li>This will better demonstrate the process of transforming data from bronze to silver and then from silver to gold.</li> <li>And it makes it easier to do debugging, troubleshooting, and for reuse.</li> </ul> <ol> <li> <p>Return to the workspace home page and create a new notebook called <code>Transform data for Gold</code></p> </li> <li> <p>In the Explorer pane, add your Sales lakehouse by selecting Add data items and then selecting the Sales lakehouse you created earlier. You should see the sales_silver table listed in the Tables section of the explorer pane.</p> </li> <li> <p>In the existing code block, remove the commented text and add the following code to load data to your dataframe and start building your star schema, then run it:</p> <pre><code># Load data to the dataframe as a starting point to create the gold layer\ndf = spark.read.table(\"Sales.sales_silver\")\n</code></pre> <p>If you receive a <code>TooManyRequestsForCapacity</code> error when running the first cell:</p> <ul> <li>Make sure you stopped the session previously running in the first notebook.</li> </ul> </li> <li> <p>Add a new code block and paste the following code to create your date dimension table and run it:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import*\n\n# Define the schema for the dimdate_gold table\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimdate_gold\") \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"Day\", IntegerType()) \\\n    .addColumn(\"Month\", IntegerType()) \\\n    .addColumn(\"Year\", IntegerType()) \\\n    .addColumn(\"mmmyyyy\", StringType()) \\\n    .addColumn(\"yyyymm\", StringType()) \\\n    .execute()\n</code></pre> <p>Note</p> <ul> <li>You can run the <code>display(df)</code> command at any time to check the progress of your work.</li> <li>In this case, you'd run <code>display(dfdimDate_gold)</code> to see the contents of the dimDate_gold dataframe.</li> </ul> </li> <li> <p>In a new code block, add and run the following code to create a dataframe for your date dimension, dimdate_gold:</p> <pre><code>from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n\n# Create dataframe for dimDate_gold\n\ndfdimDate_gold = df.dropDuplicates([\"OrderDate\"]).select(col(\"OrderDate\"), \\\n        dayofmonth(\"OrderDate\").alias(\"Day\"), \\\n        month(\"OrderDate\").alias(\"Month\"), \\\n        year(\"OrderDate\").alias(\"Year\"), \\\n        date_format(col(\"OrderDate\"), \"MMM-yyyy\").alias(\"mmmyyyy\"), \\\n        date_format(col(\"OrderDate\"), \"yyyyMM\").alias(\"yyyymm\"), \\\n    ).orderBy(\"OrderDate\")\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimDate_gold.head(10))\n</code></pre> </li> <li> <p>You're separating the code out into new code blocks so that you can understand and watch what's happening in the notebook as you transform the data. In another new code block, add and run the following code to update the date dimension as new data comes in:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')\n\ndfUpdates = dfdimDate_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.OrderDate = updates.OrderDate'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"OrderDate\": \"updates.OrderDate\",\n    \"Day\": \"updates.Day\",\n    \"Month\": \"updates.Month\",\n    \"Year\": \"updates.Year\",\n    \"mmmyyyy\": \"updates.mmmyyyy\",\n    \"yyyymm\": \"updates.yyyymm\"\n    }\n) \\\n.execute()\n</code></pre> <p>The date dimension is now set up. Now you'll create your customer dimension.</p> </li> <li> <p>To build out the customer dimension table, add a new code block, paste and run the following code:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\n# Create customer_gold dimension delta table\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimcustomer_gold\") \\\n    .addColumn(\"CustomerName\", StringType()) \\\n    .addColumn(\"Email\",  StringType()) \\\n    .addColumn(\"First\", StringType()) \\\n    .addColumn(\"Last\", StringType()) \\\n    .addColumn(\"CustomerID\", LongType()) \\\n    .execute()\n</code></pre> </li> <li> <p>In a new code block, add and run the following code to drop duplicate customers, select specific columns, and split the \"CustomerName\" column to create \"First\" and \"Last\" name columns:</p> <pre><code>from pyspark.sql.functions import col, split\n\n# Create customer_silver dataframe\n\ndfdimCustomer_silver = df.dropDuplicates([\"CustomerName\",\"Email\"]).select(col(\"CustomerName\"),col(\"Email\")) \\\n    .withColumn(\"First\",split(col(\"CustomerName\"), \" \").getItem(0)) \\\n    .withColumn(\"Last\",split(col(\"CustomerName\"), \" \").getItem(1)) \n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimCustomer_silver.head(10))\n</code></pre> <p>Here you have created a new DataFrame <code>dfdimCustomer_silver</code> by performing various transformations such as dropping duplicates, selecting specific columns, and splitting the \"CustomerName\" column to create \"First\" and \"Last\" name columns.</p> <p>The result is a DataFrame with cleaned and structured customer data, including separate \"First\" and \"Last\" name columns extracted from the \"CustomerName\"\" column.</p> </li> <li> <p>Next we'll create the ID column for our customers. In a new code block, paste and run the following:</p> <pre><code>from pyspark.sql.functions import monotonically_increasing_id, col, when, coalesce, max, lit\n\ndfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\n\nMAXCustomerID = dfdimCustomer_temp.select(coalesce(max(col(\"CustomerID\")),lit(0)).alias(\"MAXCustomerID\")).first()[0]\n\ndfdimCustomer_gold = dfdimCustomer_silver.join(dfdimCustomer_temp,(dfdimCustomer_silver.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email), \"left_anti\")\n\ndfdimCustomer_gold = dfdimCustomer_gold.withColumn(\"CustomerID\",monotonically_increasing_id() + MAXCustomerID + 1)\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimCustomer_gold.head(10))\n</code></pre> <p>Here you're cleaning and transforming customer data (<code>dfdimCustomer_silver</code>) by performing a left anti join to exclude duplicates that already exist in the <code>dimCustomer_gold</code> table, and then generating unique CustomerID values using the <code>monotonically_increasing_id()</code> function.</p> </li> <li> <p>Now you'll ensure that your customer table remains up-to-date as new data comes in.</p> <p>In a new code block, paste and run the following:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')\n\ndfUpdates = dfdimCustomer_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.CustomerName = updates.CustomerName AND gold.Email = updates.Email'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"CustomerName\": \"updates.CustomerName\",\n    \"Email\": \"updates.Email\",\n    \"First\": \"updates.First\",\n    \"Last\": \"updates.Last\",\n    \"CustomerID\": \"updates.CustomerID\"\n    }\n) \\\n.execute()\n</code></pre> </li> <li> <p>Now you'll repeat those steps to create your product dimension. </p> <p>In a new code block, paste and run the following:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.dimproduct_gold\") \\\n    .addColumn(\"ItemName\", StringType()) \\\n    .addColumn(\"ItemID\", LongType()) \\\n    .addColumn(\"ItemInfo\", StringType()) \\\n    .execute()\n</code></pre> </li> <li> <p>Add another code block to create the product_silver dataframe.</p> <pre><code>from pyspark.sql.functions import col, split, lit, when\n\n# Create product_silver dataframe\n\ndfdimProduct_silver = df.dropDuplicates([\"Item\"]).select(col(\"Item\")) \\\n    .withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimProduct_silver.head(10))\n</code></pre> </li> <li> <p>Now you'll create IDs for your dimProduct_gold table. </p> <p>Add the following syntax to a new code block and run it:</p> <pre><code>from pyspark.sql.functions import monotonically_increasing_id, col, lit, max, coalesce\n\n#dfdimProduct_temp = dfdimProduct_silver\ndfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n\nMAXProductID = dfdimProduct_temp.select(coalesce(max(col(\"ItemID\")),lit(0)).alias(\"MAXItemID\")).first()[0]\n\ndfdimProduct_gold = dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName == dfdimProduct_temp.ItemName) &amp; (dfdimProduct_silver.ItemInfo == dfdimProduct_temp.ItemInfo), \"left_anti\")\n\ndfdimProduct_gold = dfdimProduct_gold.withColumn(\"ItemID\",monotonically_increasing_id() + MAXProductID + 1)\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dfdimProduct_gold.head(10))\n</code></pre> <p>This calculates the next available product ID based on the current data in the table, assigns these new IDs to the products, and then displays the updated product information.</p> </li> <li> <p>Similar to what you've done with your other dimensions, you need to ensure that your product table remains up-to-date as new data comes in.</p> <p>In a new code block, paste and run the following:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')\n\ndfUpdates = dfdimProduct_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n        dfUpdates.alias('updates'),\n        'gold.ItemName = updates.ItemName AND gold.ItemInfo = updates.ItemInfo'\n        ) \\\n        .whenMatchedUpdate(set =\n        {\n\n        }\n        ) \\\n        .whenNotMatchedInsert(values =\n        {\n        \"ItemName\": \"updates.ItemName\",\n        \"ItemInfo\": \"updates.ItemInfo\",\n        \"ItemID\": \"updates.ItemID\"\n        }\n        ) \\\n        .execute()\n</code></pre> <p>Now that you have your dimensions built out, the final step is to create the fact table.</p> </li> <li> <p>In a new code block, paste and run the following code to create the fact table:</p> <pre><code>from pyspark.sql.types import *\nfrom delta.tables import *\n\nDeltaTable.createIfNotExists(spark) \\\n    .tableName(\"sales.factsales_gold\") \\\n    .addColumn(\"CustomerID\", LongType()) \\\n    .addColumn(\"ItemID\", LongType()) \\\n    .addColumn(\"OrderDate\", DateType()) \\\n    .addColumn(\"Quantity\", IntegerType()) \\\n    .addColumn(\"UnitPrice\", FloatType()) \\\n    .addColumn(\"Tax\", FloatType()) \\\n    .execute()\n</code></pre> </li> <li> <p>In a new code block, paste and run the following code to create a new dataframe to combine sales data with customer and product information include customer ID, item ID, order date, quantity, unit price, and tax:</p> <pre><code>from pyspark.sql.functions import col\n\ndfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\ndfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n\ndf = df.withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \\\n\n# Create Sales_gold dataframe\n\ndffactSales_gold = df.alias(\"df1\").join(dfdimCustomer_temp.alias(\"df2\"),(df.CustomerName == dfdimCustomer_temp.CustomerName) &amp; (df.Email == dfdimCustomer_temp.Email), \"left\") \\\n        .join(dfdimProduct_temp.alias(\"df3\"),(df.ItemName == dfdimProduct_temp.ItemName) &amp; (df.ItemInfo == dfdimProduct_temp.ItemInfo), \"left\") \\\n    .select(col(\"df2.CustomerID\") \\\n        , col(\"df3.ItemID\") \\\n        , col(\"df1.OrderDate\") \\\n        , col(\"df1.Quantity\") \\\n        , col(\"df1.UnitPrice\") \\\n        , col(\"df1.Tax\") \\\n    ).orderBy(col(\"df1.OrderDate\"), col(\"df2.CustomerID\"), col(\"df3.ItemID\"))\n\n# Display the first 10 rows of the dataframe to preview your data\n\ndisplay(dffactSales_gold.head(10))\n</code></pre> </li> <li> <p>Now you'll ensure that sales data remains up-to-date by running the following code in a new code block:</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')\n\ndfUpdates = dffactSales_gold\n\ndeltaTable.alias('gold') \\\n.merge(\n    dfUpdates.alias('updates'),\n    'gold.OrderDate = updates.OrderDate AND gold.CustomerID = updates.CustomerID AND gold.ItemID = updates.ItemID'\n) \\\n.whenMatchedUpdate(set =\n    {\n\n    }\n) \\\n.whenNotMatchedInsert(values =\n    {\n    \"CustomerID\": \"updates.CustomerID\",\n    \"ItemID\": \"updates.ItemID\",\n    \"OrderDate\": \"updates.OrderDate\",\n    \"Quantity\": \"updates.Quantity\",\n    \"UnitPrice\": \"updates.UnitPrice\",\n    \"Tax\": \"updates.Tax\"\n    }\n) \\\n.execute()\n</code></pre> <p>Here you're using Delta Lake's merge operation to synchronize and update the factsales_gold table with new sales data (<code>dffactSales_gold</code>). The operation compares the order date, customer ID, and item ID between the existing data (silver table) and the new data (updates DataFrame), updating matching records and inserting new records as needed.</p> </li> </ol> <p>You now have a curated, modeled gold layer that can be used for reporting and analysis.</p>"},{"location":"03b-medallion-lakehouse/instructions/#step-7-create-a-semantic-model-optional","title":"Step 7: Create a semantic model (Optional)","text":"<p>Before you can create relationships in a semantic model, you need to have a PowerBI Pro licence</p>"},{"location":"03b-medallion-lakehouse/instructions/#get-a-powerbi-licence","title":"Get a PowerBi licence","text":"<ul> <li>Click your Workspace</li> <li>In the lakehouse row - hover over semantic model</li> <li>Click the 3 dots ...</li> <li>Click Create report</li> </ul> <p>A message box should popup offering you a PowerBi Pro trial.</p> <p>You should now be able to create the semantic model as depicted below.</p> <p>In your workspace, you can now use the gold layer to create a report and analyze the data. You can access the semantic model directly in your workspace to create relationships and measures for reporting.</p> <p>Note that you can't use the default semantic model that is automatically created when you create a lakehouse. You must create a new semantic model that includes the gold tables you created in this exercise, from the Explorer.</p> <ol> <li> <p>In your workspace, navigate to your Sales lakehouse.</p> </li> <li> <p>Select New semantic model from the ribbon of the Explorer view.</p> </li> <li> <p>Assign the name Sales_Gold to your new semantic model.</p> </li> <li> <p>Select your transformed gold tables to include in your semantic model and select Confirm.</p> <ul> <li>dimcustomer_gold</li> <li>dimdate_gold</li> <li>dimproduct_gold</li> <li>factsales_gold</li> </ul> <p>This will open the semantic model in Fabric where you can create relationships and measures, as shown here:</p> <p></p> </li> </ol>"},{"location":"03b-medallion-lakehouse/instructions/#create-a-powerbi-report","title":"Create a PowerBi report","text":"<ul> <li>You can now create new report</li> <li>Or just auto create a report: Explore &gt; Auto create report</li> </ul> <p>From here you can create reports and dashboards based on the data in your lakehouse. These reports will be connected directly to the gold layer of your lakehouse, so they'll always reflect the latest data.</p>"},{"location":"03b-medallion-lakehouse/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you've learned how to create a medallion architecture in a Microsoft Fabric lakehouse.</p> <p>If you've finished exploring the medallion architecture, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03b-medallion-lakehouse.html </p>"},{"location":"04-ingest-pipeline/","title":"Lab 04 - Ingest Data with a Pipeline in Microsoft Fabric","text":"<p>A data lakehouse is a common analytical data store for cloud-scale analytics solutions. One of the core tasks of a data engineer is to implement and manage the ingestion of data from multiple operational data sources into the lakehouse. In Microsoft Fabric, you can implement extract, transform, and load (ETL) or extract, load, and transform (ELT) solutions for data ingestion through the creation of pipelines.</p> <p>Fabric also supports Apache Spark, enabling you to write and run code to process data at scale. By combining the pipeline and Spark capabilities in Fabric, you can implement complex data ingestion logic that copies data from external sources into the OneLake storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis.</p> <p>This lab will take approximately 45 minutes to complete.</p>"},{"location":"04-ingest-pipeline/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<ul> <li>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</li> </ul>"},{"location":"04-ingest-pipeline/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<ul> <li>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</li> </ul>"},{"location":"04-ingest-pipeline/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<ul> <li>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</li> </ul>"},{"location":"04-ingest-pipeline/#step-4-create-a-pipeline","title":"Step 4: Create a pipeline","text":"<ul> <li>A simple way to ingest data is to use a Copy Data activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.</li> <li>For this exercise we will copy a <code>sales.csv</code> file</li> </ul>"},{"location":"04-ingest-pipeline/#step-5-create-a-notebook","title":"Step 5: Create a notebook","text":"<ul> <li>The next step is to load the data from the <code>sales.csv</code> file that was ingested by the Copy Data activity, apply some transformation logic, and save the transformed data as a table. </li> <li>You will append the data if the table already exists.</li> </ul>"},{"location":"04-ingest-pipeline/#step-6-modify-the-pipeline","title":"Step 6: Modify the pipeline","text":"<ul> <li>Now that you've implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.</li> </ul>"},{"location":"04-ingest-pipeline/instructions/","title":"Lab 04 ~ Ingest Data with a Pipeline in Microsoft Fabric","text":"<p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"04-ingest-pipeline/instructions/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"04-ingest-pipeline/instructions/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"04-ingest-pipeline/instructions/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a uniquename of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new lakehouse with no Tables or Files will be created.</p> <p></p> </li> <li> <p>On the Explorer pane on the left, in the ... menu for the Files node, select New subfolder and create a subfolder named new_data</p> </li> </ol>"},{"location":"04-ingest-pipeline/instructions/#step-4-create-a-pipeline","title":"Step 4: Create a pipeline","text":"<p>A simple way to ingest data is to use a Copy Data activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, select Get data and then select New data pipeline, and create a new data pipeline named <code>Ingest Sales Data</code></p> </li> <li> <p>If the Copy Data wizard doesn't open automatically, select Copy Data &gt; Use copy assistant in the pipeline editor page.</p> </li> <li> <p>In the Copy Data wizard, on the Choose data source page, type HTTP in the search bar and then select HTTP in the New sources section.</p> <p></p> </li> <li> <p>In the Connect to data source pane, enter the following settings for the connection to your data source:</p> <ul> <li>URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv</li> <li>Connection: Create new connection</li> <li>Connection name: Specify a unique name</li> <li>Data gateway: (none)</li> <li>Authentication kind: Anonymous</li> </ul> </li> <li> <p>Select Next. Then ensure the following settings are selected:</p> <ul> <li>Relative URL: Leave blank</li> <li>Request method: GET</li> <li>Additional headers: Leave blank</li> <li>Binary copy: Unselected</li> <li>Request timeout: Leave blank</li> <li>Max concurrent connections: Leave blank</li> </ul> </li> <li> <p>Select Next, and wait for the data to be sampled and then ensure that the following settings are selected:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>First row as header: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>Select Preview data to see a sample of the data that will be ingested. Then close the data preview and select Next.</p> </li> <li> <p>On the Connect to data destination page, set the following data destination options, and then select Next:</p> <ul> <li>Root folder: Files</li> <li>Folder path name: new_data</li> <li>File name: sales.csv</li> <li>Copy behavior: None</li> </ul> </li> <li> <p>Set the following file format options and then select Next:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>Add header to file: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>On the Copy summary page, review the details of your copy operation and then select Save + Run.</p> <p>A new pipeline containing a Copy Data activity is created, as shown here:</p> <p></p> </li> <li> <p>When the pipeline starts to run, you can monitor its status in the Output pane under the pipeline designer. Use the  (Refresh) icon to refresh the status, and wait until it has succeeeded.</p> </li> <li> <p>In the menu bar on the left, select your lakehouse.</p> </li> <li> <p>On the Home page, in the Explorer pane, expand Files and select the new_data folder to verify that the sales.csv file has been copied.</p> </li> </ol>"},{"location":"04-ingest-pipeline/instructions/#step-5-create-a-notebook","title":"Step 5: Create a notebook","text":"<ol> <li> <p>On the Home page for your lakehouse, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> </li> <li> <p>Select the existing cell in the notebook, which contains some simple code, and then replace the default code with the following variable declaration.</p> <pre><code>table_name = \"sales\"\n</code></pre> </li> <li> <p>In the ... menu for the cell (at its top-right) select Toggle parameter cell. This configures the cell so that the variables declared in it are treated as parameters when running the notebook from a pipeline.</p> </li> <li> <p>Under the parameters cell, use the + Code button to add a new code cell. Then add the following code to it:</p> <pre><code>from pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n</code></pre> <p>This code loads the data from the sales.csv file that was ingested by the Copy Data activity, applies some transformation logic, and saves the transformed data as a table - appending the data if the table already exists.</p> </li> <li> <p>Verify that your notebooks looks similar to this, and then use the  Run all button on the toolbar to run all of the cells it contains.</p> <p></p> <p>Note</p> <ul> <li>Since this is the first time you've run any Spark code in this session, the Spark pool must be started.</li> <li>This means that the first cell can take a minute or so to complete.</li> </ul> </li> <li> <p>When the notebook run has completed, in the Explorer pane on the left, in the ... menu for Tables select Refresh and verify that a sales table has been created.</p> </li> <li> <p>In the notebook menu bar, use the \u2699\ufe0f Settings icon to view the notebook settings. Then set the Name of the notebook to <code>Load Sales</code> and close the settings pane.</p> </li> <li> <p>In the hub menu bar on the left, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, refresh the view. Then expand Tables, and select the sales table to see a preview of the data it contains.</p> </li> </ol>"},{"location":"04-ingest-pipeline/instructions/#step-6-modify-the-pipeline","title":"Step 6: Modify the pipeline","text":"<p>Now that you've implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.</p> <ol> <li> <p>In the hub menu bar on the left select the Ingest Sales Data pipeline you created previously.</p> </li> <li> <p>On the Activities tab, in the All activities list, select Delete data. Then position the new Delete data activity to the left of the Copy data activity and connect its On completion output to the Copy data activity, as shown here:</p> <p></p> </li> <li> <p>Select the Delete data activity, and in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Delete old files</code></li> </ul> </li> <li> <p>Source:</p> <ul> <li>Connection: Your lakehouse</li> <li>File path type: Wildcard file path</li> <li>Folder path: Files / new_data</li> <li>Wildcard file name: <code>*.csv</code></li> <li>Recursively: Selected</li> </ul> </li> <li> <p>Logging settings:</p> <ul> <li>Enable logging: Unselected</li> </ul> </li> </ul> <p>These settings will ensure that any existing .csv files are deleted before copying the sales.csv file.</p> </li> <li> <p>In the pipeline designer, on the Activities tab, select Notebook to add a Notebook activity to the pipeline.</p> </li> <li> <p>Select the Copy data activity and then connect its On Completion output to the Notebook activity as shown here:</p> <p></p> </li> <li> <p>Select the Notebook activity, and then in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Load Sales notebook</code></li> </ul> </li> <li> <p>Settings:</p> <ul> <li>Notebook: Load Sales</li> <li>Base parameters: Add a new parameter with the following properties:</li> </ul> Name Type Value table_name String new_sales </li> </ul> <p>The table_name parameter will be passed to the notebook and override the default value assigned to the table_name variable in the parameters cell.</p> </li> <li> <p>On the Home tab, use the  (Save) icon to save the pipeline. Then use the  Run button to run the pipeline, and wait for all of the activities to complete.</p> <p></p> <p>If you see an error message</p> <ul> <li>In case you receive the error message:<ul> <li>Spark SQL queries are only possible in the context of a lakehouse. Please attach a lakehouse to proceed:</li> </ul> </li> <li>Open your notebook, select the lakehouse you created on the left pane,</li> <li>select Remove all Lakehouses and then add it again.</li> <li>Go back to the pipeline designer and select  Run.</li> </ul> </li> <li> <p>In the hub menu bar on the left edge of the portal, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, expand Tables and select the new_sales table to see a preview of the data it contains. This table was created by the notebook when it was run by the pipeline.</p> </li> </ol> <p>In this exercise, you implemented a data ingestion solution that uses a pipeline to copy data to your lakehouse from an external source, and then uses a Spark notebook to transform the data and load it into a table.</p>"},{"location":"04-ingest-pipeline/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>Once you've finished exploring your pipeline, you should delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/04-ingest-pipeline.html </p>"},{"location":"05-dataflows-gen2/","title":"Lab 05 - Create and use Dataflows (Gen2) in Microsoft Fabric","text":"<p>In Microsoft Fabric, Dataflows (Gen2) connect to various data sources and perform transformations in Power Query Online. They can then be used in Data Pipelines to ingest data into a lakehouse or other analytical store, or to define a dataset for a Power BI report.</p> <p>This lab is designed to introduce the different elements of Dataflows (Gen2), and not create a complex solution that may exist in an enterprise. This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"05-dataflows-gen2/#overview","title":"Overview","text":""},{"location":"05-dataflows-gen2/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p>"},{"location":"05-dataflows-gen2/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<ul> <li>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</li> <li>When your new workspace opens, it will be empty.</li> </ul>"},{"location":"05-dataflows-gen2/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p>"},{"location":"05-dataflows-gen2/#step-4-create-a-dataflow-gen2-to-ingest-data","title":"Step 4: Create a Dataflow (Gen2) to ingest data","text":"<p>Now that you have a lakehouse, you need to ingest some data into it. One way to do this is to define a dataflow that encapsulates an extract, transform, and load (ETL) process.</p>"},{"location":"05-dataflows-gen2/#step-5-transform-the-data-using-power-query","title":"Step 5: Transform the data using Power Query","text":"<p>You can now transform the data. In this lab we will add a column using a custom formula.</p>"},{"location":"05-dataflows-gen2/#step-6-add-data-destination-for-dataflow","title":"Step 6: Add data destination for Dataflow","text":"<p>The next step is to define a destination for your transformed data. This will be the lakehouse you created in Step 3.</p>"},{"location":"05-dataflows-gen2/#step-7-add-a-dataflow-to-a-pipeline","title":"Step 7: Add a dataflow to a pipeline","text":"<p>You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.</p>"},{"location":"05-dataflows-gen2/#clean-up-resources","title":"Clean up resources","text":"<p>Once you've finished exploring dataflows in Microsoft Fabric, you should delete the workspace you created for this exercise.</p>"},{"location":"05-dataflows-gen2/instructions/","title":"Lab 05 ~ Create and use Dataflows (Gen2) in Microsoft Fabric","text":"<p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"05-dataflows-gen2/instructions/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-4-create-a-dataflow-gen2-to-ingest-data","title":"Step 4: Create a Dataflow (Gen2) to ingest data","text":"<p>Now that you have a lakehouse, you need to ingest some data into it. One way to do this is to define a dataflow that encapsulates an extract, transform, and load (ETL) process.</p> <ol> <li> <p>In the home page for your lakehouse, select Get data &gt; New Dataflow Gen2</p> <p></p> <p>Click Create, and after a few seconds, the Power Query editor for your new dataflow opens as shown here:</p> <p></p> </li> <li> <p>Select Import from a Text/CSV file, and create a new data source with the following settings:</p> <ul> <li>Link to file: Selected</li> <li>File path or URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv</li> <li>Connection: Create new connection</li> <li>Connection Name: default value ~ or orders.csv if name already exists</li> <li>data gateway: (none)</li> <li>Authentication kind: Anonymous</li> <li>Privacy Level: None</li> </ul> </li> <li> <p>Select Next to preview the file data, and then Create the data source.</p> <p>The Power Query editor shows the data source and an initial set of query steps to format the data, as shown here:</p> <p></p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-5-transform-the-data-using-power-query","title":"Step 5: Transform the data using Power Query","text":"<p>You can now transform the data. In this lab we will add a column using a custom formula.</p> <ol> <li> <p>On the toolbar ribbon, select the Add column tab. Then select Custom column and create a new column.</p> <p></p> </li> <li> <p>Do the following:</p> <ul> <li>Set the New column name to: MonthNo</li> <li>Set the Data type to: Whole number</li> <li>Add this Custom column formula: <code>Date.Month([OrderDate])</code></li> </ul> <p></p> </li> <li> <p>Click OK to create the column. Notice how the step to add the custom column is added to the query.</p> <p>The resulting column is displayed in the data pane:</p> <p></p> <p>Info</p> <ul> <li>In the Query Settings pane on the right side, notice the Applied Steps include each transformation step.</li> <li>At the bottom, you can also toggle the Diagram view button to turn on the Visual Diagram of the steps.</li> </ul> <p>Info</p> <p>Steps can be moved up or down, edited by selecting the gear icon, and you can select each step to see the transformations apply in the preview pane.</p> </li> <li> <p>Check and confirm that the data type for the OrderDate column is set to Date and the data type for the newly created column MonthNo is set to Whole Number.</p> <p></p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-6-add-data-destination-for-dataflow","title":"Step 6: Add data destination for Dataflow","text":"<p>The next step is to define a destination for your transformed data. This will be the lakehouse you created in Step 3.</p> <ol> <li> <p>On the toolbar ribbon, select the Home tab. Then in the Add data destination drop-down menu, select Lakehouse.</p> <p>Note</p> <ul> <li>If this option is grayed out, you may already have a data destination set.</li> <li>Check the data destination at the bottom of the Query settings pane on the right side of the Power Query editor.</li> <li>If a default destination is already set, you can remove it and add a new one.</li> </ul> </li> <li> <p>In the Connect to data destination dialog box, use the existing connection credentials:</p> <p></p> </li> <li> <p>Select Next and in the list of available workspaces, find your workspace and select the lakehouse you created in it at the start of this exercise. Then specify a new table named orders:</p> <p></p> </li> <li> <p>Select Next and on the Choose destination settings page:</p> <ul> <li>Disable the Use automatic settings option, select Append, and then Save settings.</li> </ul> <p></p> </li> <li> <p>On the Menu bar, open View and select Diagram view. Notice the Lakehouse destination is indicated as an icon in the query in the Power Query editor.</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Home tab. Then select Save &amp; run and wait for the Dataflow 1 dataflow to be created in your workspace.</p> </li> </ol>"},{"location":"05-dataflows-gen2/instructions/#step-7-add-a-dataflow-to-a-pipeline","title":"Step 7: Add a dataflow to a pipeline","text":"<p>You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.</p> <ol> <li> <p>From your Fabric-enabled workspace, select + New item &gt; Data pipeline</p> <ul> <li>When prompted, create a new pipeline named: Load data</li> </ul> <p>Click Create, and the pipeline editor will open:</p> <p></p> <p>If the Copy Data wizard opens automatically, you can just close it.</p> </li> <li> <p>Select Pipeline activity, and add a Dataflow activity to the pipeline.</p> </li> <li> <p>With the new Dataflow1 activity selected, on the Settings tab, in the Dataflow drop-down list, select Dataflow 1 (the data flow you created previously)</p> <p></p> </li> <li> <p>On the Home tab, save the pipeline using the  (Save) icon.</p> </li> <li> <p>Use the  Run button to run the pipeline, and wait for it to complete. It may take a few minutes.</p> <p></p> </li> <li> <p>In the menu bar on the left edge, select your lakehouse.</p> </li> <li> <p>In the ... menu for Tables, select refresh. </p> <p>Then expand Tables and select the orders table, which has been created by your dataflow.</p> <p></p> </li> </ol> Tip for Power Bi Desktop users: <ul> <li>In Power BI Desktop, you can connect directly to the data transformations done with your dataflow by using the Power BI dataflows (Legacy) connector.</li> <li>You can also make additional transformations, publish as a new dataset, and distribute with intended audience for specialized datasets.</li> </ul> <p></p>"},{"location":"05-dataflows-gen2/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>Once you've finished exploring dataflows in Microsoft Fabric, you should delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/05-dataflows-gen2.html </p>"},{"location":"06c-monitor-data-warehouse/instructions/","title":"Lab 06c ~ Monitor a data warehouse in Microsoft Fabric","text":"<p>In Microsoft Fabric, a data warehouse provides a relational database for large-scale analytics. Data warehouses in Microsoft Fabric include dynamic management views that you can use to monitor activity and queries.</p> <p>This lab will take approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"06c-monitor-data-warehouse/instructions/#signing-in-to-microsoft-fabric","title":"Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"06c-monitor-data-warehouse/instructions/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"06c-monitor-data-warehouse/instructions/#create-a-sample-data-warehouse","title":"Create a sample data warehouse","text":"<p>Now that you have a workspace, it\u2019s time to create a data warehouse.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Warehouse section, select Sample warehouse and create a new data warehouse named <code>sample-dw</code></p> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new warehouse will be created and populated with sample data for a taxi ride analysis scenario.</p> <p></p> </li> </ol>"},{"location":"06c-monitor-data-warehouse/instructions/#explore-dynamic-management-views","title":"Explore dynamic management views","text":"<p>Microsoft Fabric data warehouses include dynamic management views (DMVs), which you can use to identify current activity in the data warehouse instance.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the <code>sys.dm_exec_connections</code> DMV:</p> <pre><code>SELECT * FROM sys.dm_exec_connections;\n</code></pre> </li> <li> <p>Use the  Run button to run the SQL script and view the results, which include details of all connections to the data warehouse.</p> </li> <li> <p>Modify the SQL code to query the <code>sys.dm_exec_sessions</code> DMV, like this:</p> <pre><code>SELECT * FROM sys.dm_exec_sessions;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all authenticated sessions.</p> </li> <li> <p>Modify the SQL code to query the sys.dm_exec_requests DMV, like this:</p> <pre><code>SELECT * FROM sys.dm_exec_requests;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all requests being executed in the data warehouse.</p> </li> <li> <p>Modify the SQL code to join the DMVs and return information about currently running requests in the same database, like this:</p> <pre><code>SELECT connections.connection_id,\n       sessions.session_id, sessions.login_name, sessions.login_time,\n       requests.command, requests.start_time, requests.total_elapsed_time\nFROM sys.dm_exec_connections AS connections\nINNER JOIN sys.dm_exec_sessions AS sessions\nON connections.session_id=sessions.session_id\nINNER JOIN sys.dm_exec_requests AS requests\nON requests.session_id = sessions.session_id\nWHERE requests.status = 'running'\nAND requests.database_id = DB_ID()\nORDER BY requests.total_elapsed_time DESC;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all running queries in the database (including this one).</p> </li> <li> <p>In the New SQL query drop-down list, select New SQL query to add a second query tab. Then in the new empty query tab, run the following code:</p> <pre><code>WHILE 1 = 1\n    SELECT * FROM Trip;\n</code></pre> <p>What does this code do: <code>WHILE 1 = 1</code>?</p> </li> <li> <p>Leave the query running, and return to the tab containing the code to query the DMVs and re-run it. This time, the results should include the second query that is running in the other tab. Note the elapsed time for that query.</p> </li> <li> <p>Wait a few seconds and re-run the code to query the DMVs again. The elapsed time for the query in the other tab should have increased.</p> </li> <li> <p>Return to the second query tab where the query is still running and select Cancel to cancel it.</p> </li> <li> <p>Back on the tab with the code to query the DMVs, re-run the query to confirm that the second query is no longer running.</p> </li> <li> <p>Close all query tabs.</p> </li> </ol> <p>Further Information</p> <p>See Monitor connections, sessions, and requests using DMVs in the Microsoft Fabric documentation for more information about using DMVs.</p>"},{"location":"06c-monitor-data-warehouse/instructions/#explore-query-insights","title":"Explore query insights","text":"<p>Microsoft Fabric data warehouses provide query insights - a special set of views that provide details about the queries being run in your data warehouse.</p> <ol> <li> <p>In the sample-dw data warehouse page, in the New SQL query drop-down list, select New SQL query.</p> </li> <li> <p>In the new blank query pane, enter the following Transact-SQL code to query the exec_requests_history view:</p> <pre><code>SELECT * FROM queryinsights.exec_requests_history;\n</code></pre> </li> <li> <p>Use the  Run button to run the SQL script and view the results, which include details of previously executed queries.</p> </li> <li> <p>Modify the SQL code to query the frequently_run_queries view, like this:</p> <pre><code>SELECT * FROM queryinsights.frequently_run_queries;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of frequently run queries.</p> </li> <li> <p>Modify the SQL code to query the long_running_queries view, like this:</p> <pre><code>SELECT * FROM queryinsights.long_running_queries;\n</code></pre> </li> <li> <p>Run the modified query and view the results, which show details of all queries and their durations.</p> </li> </ol> <p>Further Information</p> <p>See Query insights in Fabric data warehousing in the Microsoft Fabric documentation for more information about using query insights.</p>"},{"location":"06c-monitor-data-warehouse/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have used dynamic management views and query insights to monitor activity in a Microsoft Fabric data warehouse.</p> <p>If you've finished exploring your data warehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/06c-monitor-data-warehouse.html </p>"},{"location":"18-monitor-hub/instructions/","title":"Lab 18 ~ Monitor Fabric Activity in the Monitoring Hub","text":"<p>The monitoring hub in Microsoft Fabric provides a central place where you can monitor activity. You can use the monitoring hub to review events related to items you have permission to view.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#create-and-monitor-a-dataflow","title":"Create and monitor a Dataflow","text":"<p>In Microsoft Fabric, you can use a Dataflow (Gen2) to ingest data from a wide range of sources. In this exercise, you\u2019ll use a dataflow to get data from a CSV file and load it into a table in your lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, in the Get data menu, select New Dataflow Gen2.</p> </li> <li> <p>Name the new dataflow <code>Get Product Data</code> and select Create.</p> <p></p> </li> <li> <p>In the dataflow designer, select Import from a Text/CSV file. Then complete the Get Data wizard to create a data connection by linking to <code>https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/products.csv</code> using anonymous authentication.</p> <p>When you have completed the wizard, a preview of the data will be shown in the dataflow designer like this:</p> <p></p> </li> <li> <p>Publish the dataflow.</p> </li> <li> <p>In the navigation bar on the left, select Monitor to view the monitoring hub and observe that your dataflow is in-progress (if not, refresh the view until you see it).</p> <p></p> </li> <li> <p>Wait for a few seconds, and then refresh the page until the status of the dataflow is Succeeded.</p> </li> <li> <p>In the navigation pane, select your lakehouse. Then expand the Tables folder to verify that a table named products has been created and loaded by the dataflow (you may need to refresh the Tables folder).</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#create-and-monitor-a-spark-notebook","title":"Create and monitor a Spark notebook","text":"<p>In Microsoft Fabric, you can use notebooks to run Spark code.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Notebook.</p> <p>A new notebook named Notebook 1 is created and opened.</p> <p></p> </li> <li> <p>At the top left of the notebook, select Notebook 1 to view its details, and change its name to <code>Query Products</code></p> </li> <li> <p>In the notebook editor, in the Explorer pane, select Add data items and then select Existing data sources.</p> </li> <li> <p>Add the lakehouse you created previously.</p> </li> <li> <p>Expand the lakehouse item until you reach the products table.</p> </li> <li> <p>In the ... menu for the products table, select Load data &gt; Spark. This adds a new code cell to the notebook as shown here:</p> <p></p> </li> <li> <p>Use the  Run all button to run all cells in the notebook. It will take a moment or so to start the Spark session, and then the results of the query will be shown under the code cell.</p> <p></p> </li> <li> <p>On the toolbar, use the   (Stop session) button to stop the Spark session.</p> </li> <li> <p>In the navigation bar, select Monitor to view the monitoring hub, and note that the notebook activity is listed.</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#monitor-history-for-an-item","title":"Monitor history for an item","text":"<p>Some items in a workspace might be run multiple times. You can use the monitoring hub to view their run history.</p> <ol> <li> <p>In the navigation bar, return to the page for your workspace. Then use the  (Refresh now) button for your Get Product Data dataflow to re-run it.</p> </li> <li> <p>In the navigation pane, select the Monitor page to view the monitoring hub and verify that the dataflow is in-progress.</p> </li> <li> <p>In the ... menu for the Get Product Data dataflow, select Historical runs to view the run history for the dataflow:</p> <p></p> </li> <li> <p>In the ... menu for any of the historical runs select View detail to see details of the run.</p> </li> <li> <p>Close the Details pane and use the Back to main view button to return to the main monitoring hub page.</p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#customize-monitoring-hub-views","title":"Customize monitoring hub views","text":"<p>In this exercise you\u2019ve only run a few activities, so it should be fairly easy to find events in the monitoring hub. However, in a real environment you may need to search through a large number of events. Using filters and other view customizations can make this easier.</p> <ol> <li> <p>In the monitoring hub, use the Filter button to apply the following filter:</p> <ul> <li>Status: Succeeeded</li> <li>Item type: Dataflow Gen2</li> </ul> <p>With the filter applied, only successful runs of dataflows are listed.</p> <p></p> </li> <li> <p>Use the Column Options button to include the following columns in the view (use the Apply button to apply the changes):</p> <ul> <li>Activity name</li> <li>Status</li> <li>Item type</li> <li>Start time</li> <li>Submitted by</li> <li>Location</li> <li>End time</li> <li>Duration</li> <li>Refresh type</li> </ul> <p>You may need to scroll horizontally to see all of the columns:</p> <p></p> </li> </ol>"},{"location":"18-monitor-hub/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse, a dataflow, and a Spark notebook; and you\u2019ve used the monitoring hub to view item activity.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/18-monitor-hub.html </p>"},{"location":"19-secure-data-access/instructions/","title":"Lab 19 ~ Secure data access in Microsoft Fabric","text":"<p>Microsoft Fabric has a multi-layer security model for managing data access. Security can be set for an entire workspace, for individual items, or through granular permissions in each Fabric engine. In this exercise, you secure data using workspace, and item access controls and OneLake data access roles.</p> <p>This lab takes approximately 45 minutes to complete.</p>"},{"location":"19-secure-data-access/instructions/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"19-secure-data-access/instructions/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol> <p>Note: When you create a workspace, you automatically become a member of the Workspace Admin role.</p>"},{"location":"19-secure-data-access/instructions/#create-a-data-warehouse","title":"Create a data warehouse","text":"<p>Next, create a data warehouse in the workspace you created:</p> <ol> <li> <p>Click + New Item. On the New item page, under the Store Data section, select Sample warehouse and create a new data warehouse with a name of your choice.</p> <p>After a minute or so, a new warehouse will be created:</p> <p></p> </li> </ol>"},{"location":"19-secure-data-access/instructions/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Next, create a lakehouse in the workspace you created.</p> <ol> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Select the workspace you created.</p> </li> <li> <p>In the workspace, select the + New Item button and then select Lakehouse. Create a new Lakehouse with the name of your choice.</p> </li> </ol> <p>After a minute or so, a new Lakehouse will be created:</p> <pre><code>!!! quote \"\"\n    ![Screenshot of a new lakehouse in Fabric.](../img/sample-lakehouse.png)\n</code></pre> <ol> <li>Select the Start with sample data tile and then select the Public holidays sample. After a minute or so, the lakehouse will be populated with data.</li> </ol>"},{"location":"19-secure-data-access/instructions/#apply-workspace-access-controls","title":"Apply workspace access controls","text":"<p>Workspace roles are used to control access to workspaces and the content within them. Workspace roles can be assigned when users need to see all items in a workspace, when they need to manage workspace access, or create new Fabric items, or when they need specific permissions to view, modify or share content in the workspace.  </p> <p>In this exercise, you add a user to a workspace role, apply permissions and, see what is viewable when each set of permissions is applied. You open two browsers and sign-in as different users. In one browser, you'll be a Workspace Admin and in the other, you'll sign-in as a second, less privileged user. In one browser, the Workspace Admin changes permissions for the second user and in the second browser, you're able to see the effects of changing permissions.  </p> <ol> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Next select the workspace you created.</p> </li> <li> <p>Select on Manage access on the top of the screen.</p> <p>Note: You'll see the user you're logged, who is a a member of the Workspace Admin role because you created the workspace. No other users are assigned access to the workspace yet.</p> </li> <li> <p>Next, you'll see what a user without permissions on the workspace can view. In your browser, open an InPrivate window. In the Microsoft Edge browser, select the ellipse at the top right corner and select New InPrivate Window.</p> </li> <li> <p>Enter https://app.fabric.microsoft.com and sign-in as the second user you're using for testing.  </p> </li> <li> <p>On the bottom left corner of your screen, select Microsoft Fabric and then select Data Warehouse. Next select Workspaces (the icon looks similar to \ud83d\uddc7).  </p> <p>Note: The second user doesn't have access to the workspace, so it's not viewable.</p> </li> <li> <p>Next, you assign the Workspace Viewer role to the second user and see that the role grants read access to the warehouse in the workspace.  </p> </li> <li> <p>Return to the browser window where you're logged in as the Workspace Admin. Ensure you're still on the page that shows the workspace you created. It should have your new workspace items, and the sample warehouse and lakehouse, listed at the bottom of the page.</p> </li> <li> <p>Select Manage access at the top right of the screen.</p> </li> <li> <p>Select Add people or groups. Enter the email of the second user you're testing with. Select Add to assign the user to the workspace Viewer role.</p> </li> <li> <p>Return to the InPrivate browser window where you're logged in as the second user and select refresh button on the browser to refresh session permissions assigned to the second user.</p> </li> <li> <p>Select the Workspaces icon on the left menu bar (the icon looks similar to \ud83d\uddc7) and select on the workspace name you created as the Workspace Admin user. The second user can now see all of the items in the workspace because they were assigned the Workspace Viewer role.</p> <p></p> </li> <li> <p>Select the warehouse and open it.</p> </li> <li> <p>Select the Date table and wait for the rows to be loaded. You can see the rows because as a member of the Workspace Viewer role, you have CONNECT and ReadData permission on tables in the warehouse. For more information on permissions granted to the Workspace Viewer role, see Workspace roles.</p> </li> <li> <p>Next, select the Workspaces icon on the left menu bar, then select the lakehouse.</p> </li> <li> <p>When the lakehouse opens, click on the dropdown box at the top right corner of the screen that says Lakehouse and select SQL analytics endpoint.</p> </li> <li> <p>Select the publicholidays table and wait for the data to be displayed. Data in the lakehouse table is readable from the SQL analytics endpoint because the user is a member of the Workspace Viewer role that grants read permissions on the SQL analytics endpoint.</p> </li> </ol>"},{"location":"19-secure-data-access/instructions/#apply-item-access-control","title":"Apply item access control","text":"<p>Item permissions control access to individual Fabric items within a workspace, like warehouses, lakehouses and semantic models. In this exercise, you remove the Workspace Viewer permissions applied in the previous exercise and then apply item level permissions on the warehouse so a less privileged user can only view the warehouse data, not the lakehouse data.</p> <ol> <li> <p>Return to the browser window where you're logged in as the Workspace Admin. Select Workspaces from the left navigation pane. </p> </li> <li> <p>Select the workspace that you created to open it.</p> </li> <li> <p>Select Manage access from the top of the screen.</p> </li> <li> <p>Select the word Viewer under the name of the second user. On the menu that appears, select Remove.</p> <p></p> </li> <li> <p>Close the Manage access section.</p> </li> <li> <p>In the workspace, hover over the name of your warehouse and an ellipse (...) will appear. Select the ellipse and select Manage permissions</p> </li> <li> <p>Select Add user and enter the name of the second user.</p> </li> <li> <p>In the box that appears, under Additional permissions check Read all data using SQL (ReadData) and uncheck all other boxes.</p> <p></p> </li> <li> <p>Select Grant</p> </li> <li> <p>Return to the browser window where you're logged in as the second user. Refresh the browser view.</p> </li> <li> <p>The second user no longer has access to the workspace and instead has access to only the warehouse. You can no longer browse workspaces on the left navigation pane to find the warehouse. Select OneLake on the left navigation menu to find the warehouse. </p> </li> <li> <p>Select the warehouse. On the screen that appears, select Open from the top menu bar.</p> </li> <li> <p>When the warehouse view appears, select the Date table to view table data. The rows are viewable because the user still has read access to the warehouse because ReadData permissions were applied by using item permissions on the warehouse.</p> </li> </ol>"},{"location":"19-secure-data-access/instructions/#apply-onelake-data-access-roles-in-a-lakehouse","title":"Apply OneLake data access roles in a Lakehouse","text":"<p>OneLake data access roles let you create custom roles within a Lakehouse and grant read permissions to folders you specify. OneLake data access roles is currently a Preview feature.</p> <p>In this exercise, you assign an item permission and create a OneLake data access role and experiment with how they work together to restrict access to data in a Lakehouse.  </p> <ol> <li> <p>Stay in the browser where you're logged in as the second user.  </p> </li> <li> <p>Select OneLake on the left navigation bar. The second user doesn't see the lakehouse.  </p> </li> <li> <p>Return to the browser where you're logged in as the Workspace Admin.</p> </li> <li> <p>Select Workspaces on the left menu and select your workspace. Hover over the name of the lakehouse.  </p> </li> <li> <p>Select on the ellipse (...) to the right of the ellipse and select Manage permissions</p> <p></p> </li> <li> <p>On the screen that appears, select Add user.</p> </li> <li> <p>Assign the second user to the lakehouse and ensure none of the checkboxes on the Grant People Access window are checked.  </p> <p></p> </li> <li> <p>Select Grant. The second user now has read permissions on the lakehouse. Read permission only allows the user to see metadata for the lakehouse but not the underlying data. Next we'll validate this.</p> </li> <li> <p>Return to the browser where you're logged in as the second user. Refresh the browser.</p> </li> <li> <p>Select OneLake in the left navigation pane.  </p> </li> <li> <p>Select the lakehouse and open it. </p> </li> <li> <p>Select Open on the top menu bar. You're unable to expand the tables or files even though read permission was granted. Next, you grant the second user access to a specific folder using OneLake data access permissions.</p> </li> <li> <p>Return to the browser where you're logged in as the workspace administrator.</p> </li> <li> <p>Select Workspaces from the left navigation bar.</p> </li> <li> <p>Select your workspace name.</p> </li> <li> <p>Select the lakehouse.</p> <p>When the lakehouse opens, select Manage OneLake data access on the top menu bar and enable the feature by clicking the Continue button.</p> <p></p> </li> <li> <p>Select new role on the Manage OneLake data access (preview) screen that appears.</p> <p></p> </li> <li> <p>Create a new role called publicholidays that can only access the publicholidays folder as shown in the screenshot below.</p> <p></p> </li> <li> <p>When the role finishes creating, select Assign role and assign the role to your second user, select Add and, select Save.</p> <p></p> </li> <li> <p>Return to the browser where you're logged in as the second user. Ensure you're still on the page where the lakehouse is open. Refresh the browser.  </p> </li> <li> <p>Select the publicholidays table and wait for the data to load. Only the data in the publicholidays table is accessible to the user because the user was assigned to the custom OneLake data access role. The role permits them to see only the data in the publicholidays table, not data in any of the other tables, files, or folders.</p> </li> </ol>"},{"location":"19-secure-data-access/instructions/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you secured data using workspace access controls, item access controls and, OneLake data access roles.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol>"},{"location":"21-deployment-pipelines/instructions/","title":"Lab 21 ~ Implement deployment pipelines in Microsoft Fabric","text":"<p>Deployment pipelines in Microsoft Fabric let you automate the process of copying   changes made to the content in Fabric items between environments like development, test, and production. You can use deployment pipelines to develop and test content before it reaches end users. In this exercise, you create a deployment pipeline, and assign stages to the pipeline. Then you create some content in a development workspace and use deployment pipelines to deploy it between the Development, Test and Production pipeline stages.</p> <p>Note: To complete this exercise, you need to be an member of the Fabric workspace admin role. To assign roles see Roles in workspaces in Microsoft Fabric.</p> <p>This lab takes approximately 20 minutes to complete.</p>"},{"location":"21-deployment-pipelines/instructions/#create-workspaces","title":"Create workspaces","text":"<p>Create three workspaces with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page at <code>https://app.fabric.microsoft.com/home?experience=fabric</code> in a browser and sign in with your Fabric credentials.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a new workspace named Development, selecting a licensing mode that includes Fabric capacity (Trial, Premium, or Fabric).</p> </li> <li> <p>Repeat steps 1 &amp; 2, creating two more workspaces named Test, and Production. Your workspaces are: Development, Test, and Production.</p> </li> <li> <p>Select the Workspaces icon on the menu bar on the left and confirm that there are three workspaces named:  Development, Test, and Production</p> </li> </ol> <p>Note: If you are prompted to enter a unique name for the workspaces, append one or more random numbers to the words: Development, Test, or Production.</p>"},{"location":"21-deployment-pipelines/instructions/#create-a-deployment-pipeline","title":"Create a deployment pipeline","text":"<p>Next, create a deployment pipeline.</p> <ol> <li> <p>In the menu bar on the left, select Workspaces.</p> </li> <li> <p>Select Deployment Pipelines, then New pipeline.</p> </li> <li> <p>In the Add a new deployment pipeline window, give the pipeline a unique name and select Next.</p> </li> <li> <p>In the new pipeline window, select Create and continue.</p> </li> </ol>"},{"location":"21-deployment-pipelines/instructions/#assign-workspaces-to-stages-of-a-deployment-pipeline","title":"Assign workspaces to stages of a deployment pipeline","text":"<ol> <li> <p>On the left menu bar, select the pipeline you created. </p> </li> <li> <p>In the window that appears, expand the options under Assign a workspace on each deployment stage and select the name of the workspace that matches the name of the stage.</p> </li> <li> <p>Select the check mark Assign for each deployment stage.</p> <p></p> </li> </ol>"},{"location":"21-deployment-pipelines/instructions/#create-content","title":"Create content","text":"<p>Fabric items haven't been created in your workspaces yet. Next, create a lakehouse in the development workspace.</p> <ol> <li> <p>In the menu bar on the left, select Workspaces.</p> </li> <li> <p>Select the Development workspace.</p> </li> <li> <p>Select New Item.</p> </li> <li> <p>In the window that appears, select Lakehouse and in the New lakehouse window, name the lakehouse, LabLakehouse.</p> </li> <li> <p>Select Create.</p> </li> <li> <p>In the Lakehouse Explorer window, select Start with sample data to populate the new lakehouse with data.</p> <p></p> </li> <li> <p>Select the sample NYCTaxi.</p> </li> <li> <p>In the menu bar on the left, select the pipeline you created.</p> </li> <li> <p>Select the Development stage, and under the deployment pipeline canvas you can see the lakehouse you created as a stage item. In the left edge of the Test stage, there's an X within a circle. The X indicates that the Development and Test stages aren't synchronized.</p> </li> <li> <p>Select the Test stage and under the deployment pipeline canvas you can see that the lakehouse you created is only a stage item in the source, which in this case refers to the Development stage.  </p> <p></p> </li> </ol>"},{"location":"21-deployment-pipelines/instructions/#deploy-content-between-stages","title":"Deploy content between stages","text":"<p>Deploy the lakehouse from the Development stage to the Test and Production stages.</p> <ol> <li> <p>Select the Test stage in the deployment pipeline canvas.</p> </li> <li> <p>Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Test stage.</p> </li> <li> <p>In the Deploy to next stage window that appears, select Deploy.  There is now an X in a circle in the Production stage in the deployment pipeline canvas. The lakehouse exists in the Development and Test stages but not yet in the Production stage.</p> </li> <li> <p>Select the Production stage in the deployment canvas.</p> </li> <li> <p>Under the deployment pipeline canvas, select the checkbox next to the Lakehouse item. Then select the Deploy button to copy the lakehouse in its current state to the Production stage.</p> </li> <li> <p>In the Deploy to next stage window that appears, select Deploy. The green check marks between the stages indicates that all stages in sync and contain the same content.</p> </li> <li> <p>Using deployment pipelines to deploy between stages also updates the content in the workspaces corresponding to the deployment stage. Let's confirm.</p> </li> <li> <p>In the menu bar on the left, select Workspaces.</p> </li> <li> <p>Select the Test workspace. The lakehouse was copied there.</p> </li> <li> <p>Open the Production workspace from the Workspaces icon on the left menu. The lakehouse was copied to the Production workspace too.</p> </li> </ol>"},{"location":"21-deployment-pipelines/instructions/#clean-up","title":"Clean up","text":"<p>In this exercise, you created a deployment pipeline, and assigned stages to the pipeline. Then you created content in a development workspace and deployed it between pipeline stages using deployment pipelines.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol>"}]}